{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy measures amount of randomness present in probability distribution. The cross entropy tells how simillar or different two probability distributions are. You can calculate corss entropy for binary classifier using below formula.<br>\n",
    "\n",
    "$$BCE = −(ylog(p)+(1−y)log(1−p))$$\n",
    "\n",
    "The same forumla can be generalized to multiclass. It is called categorical cross entropy<br>\n",
    "\n",
    "$$CCE = -\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement binary cross entropy using pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(actual, predicted):\n",
    "    '''\n",
    "    p*log(q) + (1-p)*log(1-q)\n",
    "    '''\n",
    "    sum_ = 0\n",
    "    for p, q in zip(actual, predicted):\n",
    "        sum_ += p*math.log(q)+(1-p)*math.log(1-q)\n",
    "\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [1, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
    "predicted = [1, 0, 0, 0, 1, 1, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1f894ae03abe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss_formatted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{:.10f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5ef460075aca>\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(actual, predicted)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msum_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msum_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "loss = binary_cross_entropy(actual, predicted)\n",
    "loss_formatted = '{:.10f}'.format(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is implemented correct. However, it didn't work because log is only defined for positive numbers. If we try to get the log of zero, we will get math domain error. To get around this we have to clip the values which are zeros and ones to the closeset number of zero and one. Also, this is how the machine learning libraries implemnet cross entropy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(actual, predicted):\n",
    "    '''\n",
    "    p*log(q) + (1-p)*log(1-q)\n",
    "    '''\n",
    "    sum_ = 0\n",
    "    min_ = 1e-7\n",
    "    max_ = 1-1e-7\n",
    "    \n",
    "    for p, q in zip(actual, predicted):\n",
    "        if p < min_:\n",
    "            p = min_\n",
    "        if p > max_:\n",
    "            p = max_\n",
    "            \n",
    "        if q < min_:\n",
    "            q = min_\n",
    "        if q > max_:\n",
    "            q = max_\n",
    "            \n",
    "        sum_ += p*math.log(q)+(1-p)*math.log(1-q)\n",
    "\n",
    "    return -sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = binary_cross_entropy(actual, predicted)\n",
    "loss_formatted = '{:.10f}'.format(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7118095596453213e-05\n",
      "0.0000171181\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(loss_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of pure Python for heavy compuration is always a slow operation as compared to Numpy. The same formula can be implemented in Numpy using vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(actual, predicted):\n",
    "    '''\n",
    "    p*log(q) + (1-p)*log(1-q)\n",
    "    '''\n",
    "    actual = np.clip(actual, 1e-7, 1-1e-7)\n",
    "    predicted = np.clip(actual, 1e-7, 1-1e-7)\n",
    "    return -np.sum((actual*np.log(predicted)) + (1-actual)*np.log(1-predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0])\n",
    "predicted = np.array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = binary_cross_entropy(actual, predicted)\n",
    "loss_formatted = '{:.10f}'.format(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7118095596453213e-05\n",
      "0.0000171181\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(loss_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References<br>\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
